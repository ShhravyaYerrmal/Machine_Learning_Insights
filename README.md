# Machine_Learning_Insights

8 Feature Engineering Techniques:

Feature Engineering is relly important for improving the performance of your machinne learning models. It is absolutely necessary to decrypt how humans understands the data. Humans have tendency to find complex patterns or relations and recognize, even when they don't actually exist.

Deconstructing is an "ART"

1. Imputation:

It deals with handling missing values in data. While deleting records that are missing certain values is one way of dealing with this issue, it could also mean losing out on a chunk of valuable data. This is where imputation can help. It can be broadly classified into two types. Such as:

Categorical Imputation: Missing categorical values are generally replaced by the most commonly occurring value in other records.
Numerical Imputation: Missing numerical values are generally replaced by the mean of the corresponding value in other records.

2. Discretization

It involves essentially taking a set of values of data and grouping sets of them together in some logical fashion into bins (or buckets). Binning can apply to numerical values as well as to categorical values. This could help prevent data from overfitting but comes at the cost of loss of granularity of data. The grouping of data can be done as follows:

a) Grouping of equal intervals
b) Grouping based on equal frequencies (of observations in the bin)
c) Grouping based on decision tree sorting (to establish a relationship with target)



